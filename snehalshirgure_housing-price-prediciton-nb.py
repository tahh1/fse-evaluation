#!/usr/bin/env python
# coding: utf-8

# In[140]:


# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt 

from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

# Input data files are available in the read-only "../input/" directory
import os
# print(os.listdir("../input"))
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print((os.path.join(dirname, filename)))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session


# In[141]:


#csv to pd
path='/kaggle/input/melbourne-housing-snapshot/melb_data.csv'
data=pd.read_csv(path)
pd.set_option("display.max_columns" , 100)
pd.set_option("display.max_rows" , 100)

#generate descriptive statistics
data.describe()


# In[142]:


#size of dataset
print((data.shape))

#get no. of columns
data.columns


# In[143]:


data.head(10)


# In[144]:


#sorted list of na values in each column
data.isna().sum().sort_values(ascending=False)


# In[145]:


# dropna drops missing values (think of na as "not available")
data = data.dropna(axis=0)
data.head()


# In[146]:


#choosing features for modeling
X_final = data[['Rooms','Bedroom2','Bathroom','Car','Landsize','BuildingArea']].copy()
X_final.head()


# In[147]:


#prediction target for modeling
y_final = data['Price']
y_final.head()


# In[148]:


############################################ LinearRegression ##############################################

lr = LinearRegression().fit(X_train,y_train)
y_train_pred = lr.predict(X_train)
y_test_pred = lr.predict(X_test)

#print score
print(("lr.coef_: {}".format(lr.coef_)))
print(("lr.intercept_: {}".format(lr.intercept_)))
print(('lr train score %.3f, lr test score: %.3f' % (
lr.score(X_train,y_train),
lr.score(X_test, y_test))))


# In[149]:


############################################ PolynomialRegression ##############################################

poly = PolynomialFeatures (degree = 3)
X_poly = poly.fit_transform(X_final)

X_train,X_test,y_train,y_test = train_test_split(X_poly,y_final, test_size = 0.33, random_state = 0)

#standard scaler (fit transform on train, transform only on test)
sc = StandardScaler()
X_train = sc.fit_transform(X_train.astype(np.float))
X_test= sc.transform(X_test.astype(np.float))

#fit model
poly_lr = LinearRegression().fit(X_train,y_train)

y_train_pred = poly_lr.predict(X_train)
y_test_pred = poly_lr.predict(X_test)

#print score
print(('poly train score %.3f, poly test score: %.3f' % (
poly_lr.score(X_train,y_train),
poly_lr.score(X_test, y_test))))


# In[150]:


############################################ SupportVectorRegression ##############################################

svr = SVR(kernel='linear', C = 300)

#test train split
X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size = 0.33, random_state = 0 )

print((X_train.head()))
print((X_test.head()))
print((y_train.head()))
print((y_test.head()))

#standard scaler (fit transform on train, tranform only on test)
sc = StandardScaler()
X_train = sc.fit_transform(X_train.astype(np.float))
X_test= sc.transform(X_test.astype(np.float))

#fit model
svr = svr.fit(X_train,y_train.values.ravel())
y_train_pred = svr.predict(X_train)
y_test_pred = svr.predict(X_test)

#print score
print(('svr train score %.3f, svr test score: %.3f' % (
svr.score(X_train,y_train),
svr.score(X_test, y_test))))


# In[151]:


############################################ DecisionTree ##############################################

dt = DecisionTreeRegressor(random_state=0)

#test train split
X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size = 0.33, random_state = 0 )

print((X_train.head()))
print((X_test.head()))
print((y_train.head()))
print((y_test.head()))

#standard scaler (fit transform on train, tranform only on test)
sc = StandardScaler()
X_train = sc.fit_transform(X_train.astype(np.float))
X_test= sc.transform(X_test.astype(np.float))


#fit model
dt = dt.fit(X_train,y_train.values.ravel())
y_train_pred = dt.predict(X_train)
y_test_pred = dt.predict(X_test)

#print score
print(('dt train score %.3f, dt test score: %.3f' % (
dt.score(X_train,y_train),
dt.score(X_test, y_test))))


# In[152]:


############################################ RandomForestRegression #######################################

forest = RandomForestRegressor(n_estimators = 100,
                              criterion = 'mse',
                              random_state = 1,
                              n_jobs = -1)
#test train split
X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size = 0.33, random_state = 0 )

print((X_train.head()))
print((X_test.head()))
print((y_train.head()))
print((y_test.head()))

#standard scaler (fit transform on train, tranform only on test)
sc = StandardScaler()
X_train = sc.fit_transform(X_train.astype(np.float))
X_test= sc.transform(X_test.astype(np.float))

#fit model
forest.fit(X_train,y_train.values.ravel())
y_train_pred = forest.predict(X_train)
y_test_pred = forest.predict(X_test)

# print(y_train_pred)
# print(y_test_pred)

#print score
print(('forest train score %.3f, forest test score: %.3f' % (
forest.score(X_train,y_train),
forest.score(X_test, y_test))))

#metric 2
mae = mean_absolute_error(y_test, y_test_pred)
print(mae)


# In[153]:


#Function to print best hyperparamaters: 
def print_best_params(gd_model):
    param_dict = gd_model.best_estimator_.get_params()
    model_str = str(gd_model.estimator).split('(')[0]
    print(("\n*** {} Best Parameters ***".format(model_str)))
    for k in param_dict:
        print(("{}: {}".format(k, param_dict[k])))
    print()


# In[155]:


### Random Forest parameter grid ###

param_grid_rf = dict(n_estimators=[20],
                     max_depth=np.arange(1, 13, 2),
                     min_samples_split=[2],
                     min_samples_leaf= np.arange(1, 15, 2, int),
                     bootstrap=[True, False],
                     oob_score=[False, ])


forest = GridSearchCV(RandomForestRegressor(random_state=0), param_grid=param_grid_rf, cv=5, verbose=3)

#fit model
forest.fit(X_train,y_train.values.ravel())


#print score
print(('\n\nforest train score %.3f, forest test score: %.3f' % (
forest.score(X_train,y_train),
forest.score(X_test, y_test))))

print_best_params(forest)

